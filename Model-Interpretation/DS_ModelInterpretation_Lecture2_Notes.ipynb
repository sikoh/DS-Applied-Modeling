{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79D517qEz64r"
      },
      "source": [
        "# Model Interpretation\n",
        "\n",
        "- Visualize and interpret **partial dependence plots**\n",
        "- Explain individual predictions with **shapley value plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzrYuj-hz64s"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries:\n",
        "\n",
        "- category_encoders\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- [**pdpbox**](https://github.com/SauceCat/PDPbox)\n",
        "- plotly\n",
        "- scikit-learn\n",
        "- scipy.stats\n",
        "- [**shap**](https://github.com/slundberg/shap)\n",
        "- xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wthIV1uz64t"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/tree/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "    !pip install pdpbox\n",
        "    !pip install shap\n",
        "    !pip install matplotlib==3.7.1\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'\n",
        "\n",
        "\n",
        "# Ignore this warning: https://github.com/dmlc/xgboost/issues/4300\n",
        "# xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning, module='xgboost')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDYkwwFz64u"
      },
      "source": [
        "# Visualize and interpret partial dependence plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4dyL4d9z64v"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFL01sy3z64v"
      },
      "source": [
        "Partial dependence plots show the relationship between 1-2 individual features and the target — how predictions partially depend on the isolated features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8_uLJfUz64v"
      },
      "source": [
        "It's explained well by [PDPbox library documentation](https://pdpbox.readthedocs.io/en/latest/):\n",
        "\n",
        ">**The common headache**: When using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome. For example, in terms of random forest, all we get is the feature importance. Although we can know which feature is significantly influencing the outcome based on the importance calculation, it really sucks that we don’t know in which direction it is influencing. And in most of the real cases, the effect is non-monotonic. We need some powerful tools to help understanding the complex relations between predictors and model prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWIR4HABz64w"
      },
      "source": [
        "Let's also look at an [animation by Christoph Molnar](https://twitter.com/ChristophMolnar/status/1066398522608635904), author of [_Interpretable Machine Learning_](https://christophm.github.io/interpretable-ml-book/pdp.html#examples):\n",
        "\n",
        "> Partial dependence plots show how a feature affects predictions of a Machine Learning model on average.\n",
        "> 1. Define grid along feature\n",
        "> 2. Model predictions at grid points\n",
        "> 3. Line per data instance -> ICE (Individual Conditional Expectation) curve\n",
        "> 4. Average curves to get a PDP (Partial Dependence Plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le_fYYJ1z64w"
      },
      "source": [
        "To demonstrate, we'll use a Lending Club dataset, to predict interest rates. (Like [this example](https://rrherr-project2-example.herokuapp.com/).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-p3Udjxz64w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Stratified sample, 10% of expired Lending Club loans, grades A-D\n",
        "# Source: https://www.lendingclub.com/info/download-data.action\n",
        "history = pd.read_csv(DATA_PATH+'lending-club/lending-club-subset.csv')\n",
        "history['issue_d'] = pd.to_datetime(history['issue_d'], infer_datetime_format=True)\n",
        "\n",
        "# Just use 36 month loans\n",
        "history = history[history.term==' 36 months']\n",
        "\n",
        "# Index & sort by issue date\n",
        "history = history.set_index('issue_d').sort_index()\n",
        "\n",
        "# Clean data, engineer feature, & select subset of features\n",
        "history = history.rename(columns=\n",
        "    {'annual_inc': 'Annual Income',\n",
        "     'fico_range_high': 'Credit Score',\n",
        "     'funded_amnt': 'Loan Amount',\n",
        "     'title': 'Loan Purpose'})\n",
        "\n",
        "history['Interest Rate'] = history['int_rate'].str.strip('%').astype(float)\n",
        "history['Monthly Debts'] = history['Annual Income'] / 12 * history['dti'] / 100\n",
        "\n",
        "columns = ['Annual Income',\n",
        "           'Credit Score',\n",
        "           'Loan Amount',\n",
        "           'Loan Purpose',\n",
        "           'Monthly Debts',\n",
        "           'Interest Rate']\n",
        "\n",
        "history = history[columns]\n",
        "history = history.dropna()\n",
        "\n",
        "# Test on the last 10,000 loans,\n",
        "# Validate on the 10,000 before that,\n",
        "# Train on the rest\n",
        "test = history[-10000:]\n",
        "val = history[-20000:-10000]\n",
        "train = history[:-20000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWPhbB6zz64x"
      },
      "outputs": [],
      "source": [
        "# Assign to X, y\n",
        "target = 'Interest Rate'\n",
        "features = history.columns.drop('Interest Rate')\n",
        "\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "\n",
        "X_test = test[features]\n",
        "y_test = test[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "qQZ5szXOz64x"
      },
      "outputs": [],
      "source": [
        "# The target has some right skew, but it's not too bad\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.distplot(y_train);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IoUu14yz64y"
      },
      "source": [
        "### Fit Linear Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "wFCKb-L2z64y"
      },
      "outputs": [],
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "lr = make_pipeline(\n",
        "    ce.TargetEncoder(),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "print('Linear Regression R^2', lr.score(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QwfZMqPz642"
      },
      "source": [
        "### Explaining Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JrRycORz643"
      },
      "outputs": [],
      "source": [
        "coefficients = lr.named_steps['linearregression'].coef_\n",
        "pd.Series(coefficients, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-D0jEIJz643"
      },
      "source": [
        "### Fit Gradient Boosting model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "Y_Mifj-rz644"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "gb = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    XGBRegressor(n_estimators=200, objective='reg:squarederror', n_jobs=-1)\n",
        ")\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred = gb.predict(X_val)\n",
        "print('Gradient Boosting R^2', r2_score(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25gNLi7gz644"
      },
      "source": [
        "### Explaining Gradient Boosting???\n",
        "\n",
        "Linear models have coefficients, but trees do not.\n",
        "\n",
        "Instead, to see the relationship between individual feature(s) and the target, we can use partial dependence plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqSvgXGbz644"
      },
      "source": [
        "## Follow Along\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbIO6Uh7z645"
      },
      "source": [
        "### Partial Dependence Plots with 1 feature\n",
        "\n",
        "PDPbox\n",
        "- [Gallery](https://github.com/SauceCat/PDPbox#gallery)\n",
        "- [API Reference: pdp_isolate](https://pdpbox.readthedocs.io/en/latest/pdp_isolate.html)\n",
        "- [API Reference: pdp_plot](https://pdpbox.readthedocs.io/en/latest/pdp_plot.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPOQOtybz645"
      },
      "outputs": [],
      "source": [
        "# Later, when you save matplotlib images to include in blog posts or web apps,\n",
        "# increase the dots per inch (double it), so the text isn't so fuzzy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 72"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6zXLmu7gz645"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6k8Dns4z646"
      },
      "source": [
        "### Partial Dependence Plots with 2 features\n",
        "\n",
        "See interactions!\n",
        "\n",
        "PDPbox\n",
        "- [Gallery](https://github.com/SauceCat/PDPbox#gallery)\n",
        "- [API Reference: pdp_interact](https://pdpbox.readthedocs.io/en/latest/pdp_interact.html)\n",
        "- [API Reference: pdp_interact_plot](https://pdpbox.readthedocs.io/en/latest/pdp_interact_plot.html)\n",
        "\n",
        "Be aware of a bug in PDPBox version <= 0.20 with some versions of matplotlib:\n",
        "- With the `pdp_interact_plot` function, `plot_type='contour'` gets an error, but `plot_type='grid'` works\n",
        "- This will be fixed in the next release of PDPbox: https://github.com/SauceCat/PDPbox/issues/40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "iaNgzjAxz646"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPXAprKjz646"
      },
      "source": [
        "### BONUS: 3D with Plotly!\n",
        "\n",
        "Just for your future reference, here's how you can make it 3D! (Like [this example](https://rrherr-project2-example.herokuapp.com/).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "HsajHSzfz647"
      },
      "outputs": [],
      "source": [
        "# First, make the 2D plot above. Then ...\n",
        "\n",
        "pdp = interaction.pdp.pivot_table(\n",
        "    values='preds',\n",
        "    columns=features[0],\n",
        "    index=features[1]\n",
        ")[::-1] # Slice notation to reverse index order so y axis is ascending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFOW-WHbz647"
      },
      "outputs": [],
      "source": [
        "pdp = pdp.drop(columns=[1000.0, 751329.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRDolYQoz647"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "surface = go.Surface(\n",
        "    x=pdp.columns,\n",
        "    y=pdp.index,\n",
        "    z=pdp.values\n",
        ")\n",
        "\n",
        "\n",
        "layout = go.Layout(\n",
        "    scene=dict(\n",
        "        xaxis=dict(title=features[0]),\n",
        "        yaxis=dict(title=features[1]),\n",
        "        zaxis=dict(title=target)\n",
        "    )\n",
        ")\n",
        "\n",
        "fig = go.Figure(surface, layout)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkRlHtLJz648"
      },
      "source": [
        "### BONUS: PDPs with categorical features\n",
        "\n",
        "Just for your future reference, here's a bonus example to demonstrate partial dependence plots with categorical features.\n",
        "\n",
        "1. I recommend you use Ordinal Encoder or Target Encoder, outside of a pipeline, to encode your data first. (If there is a natural ordering, then take the time to encode it that way, instead of random integers.) Then use the encoded data with pdpbox.\n",
        "2. There's some extra work to get readable category names on your plot, instead of integer category codes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNDymOKDz648"
      },
      "outputs": [],
      "source": [
        "# Fit a model on Titanic data\n",
        "import category_encoders as ce\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "df = sns.load_dataset('titanic')\n",
        "df.age = df.age.fillna(df.age.median())\n",
        "df = df.drop(columns='deck')\n",
        "df = df.dropna()\n",
        "\n",
        "target = 'survived'\n",
        "features = df.columns.drop(['survived', 'alive'])\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Use Ordinal Encoder, outside of a pipeline\n",
        "encoder = ce.OrdinalEncoder()\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "model.fit(X_encoded, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eki8hRkxz649"
      },
      "outputs": [],
      "source": [
        "# Use Pdpbox\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from pdpbox import pdp\n",
        "feature = 'sex'\n",
        "pdp_dist = pdp.pdp_isolate(model=model, dataset=X_encoded, model_features=features, feature=feature)\n",
        "pdp.pdp_plot(pdp_dist, feature);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-31Il6Rbz649"
      },
      "outputs": [],
      "source": [
        "# Look at the encoder's mappings\n",
        "encoder.mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7FkQm1Uz64-"
      },
      "outputs": [],
      "source": [
        "pdp.pdp_plot(pdp_dist, feature)\n",
        "\n",
        "# Manually change the xticks labels\n",
        "plt.xticks([1, 2], ['male', 'female']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcCQv-rEz64_"
      },
      "outputs": [],
      "source": [
        "# Let's automate it\n",
        "\n",
        "feature = 'sex'\n",
        "for item in encoder.mapping:\n",
        "    if item['col'] == feature:\n",
        "        feature_mapping = item['mapping']\n",
        "\n",
        "feature_mapping = feature_mapping[feature_mapping.index.dropna()]\n",
        "category_names = feature_mapping.index.tolist()\n",
        "category_codes = feature_mapping.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYEW8Jeyz64_"
      },
      "outputs": [],
      "source": [
        "pdp.pdp_plot(pdp_dist, feature)\n",
        "\n",
        "# Automatically change the xticks labels\n",
        "plt.xticks(category_codes, category_names);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvgu_7hAz65A"
      },
      "outputs": [],
      "source": [
        "features = ['sex', 'age']\n",
        "\n",
        "interaction = pdp.pdp_interact(\n",
        "    model=model,\n",
        "    dataset=X_encoded,\n",
        "    model_features=X_encoded.columns,\n",
        "    features=features\n",
        ")\n",
        "\n",
        "pdp.pdp_interact_plot(interaction, plot_type='grid', feature_names=features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "YRhgkh9Iz65A"
      },
      "outputs": [],
      "source": [
        "pdp = interaction.pdp.pivot_table(\n",
        "    values='preds',\n",
        "    columns=features[0], # First feature on x axis\n",
        "    index=features[1]    # Next feature on y axis\n",
        ")[::-1]  # Reverse the index order so y axis is ascending\n",
        "\n",
        "pdp = pdp.rename(columns=dict(zip(category_codes, category_names)))\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(pdp, annot=True, fmt='.2f', cmap='viridis')\n",
        "plt.title('Partial Dependence of Titanic survival, on sex & age');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5OgeuE5z65J"
      },
      "source": [
        "# Explain individual predictions with shapley value plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYsNKR7gz65J"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE3QRky-z65J"
      },
      "source": [
        "We’ll use TreeExplainer from an awesome library called [SHAP](https://github.com/slundberg/shap), for “additive explanations” — we can explain individual predictions by seeing how the features add up!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.png\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNlm5yugz65K"
      },
      "source": [
        "### Regression example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Np0qU8z65K"
      },
      "source": [
        "We're coming full circle, with the NYC Apartment Rent dataset! Remember this code you wrote for your first assignment?\n",
        "\n",
        "```python\n",
        "# Arrange X features matrix & y target vector\n",
        "features = ['bedrooms', 'bathrooms']\n",
        "target = 'price'\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Fit model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "def predict(bedrooms, bathrooms):\n",
        "    y_pred = model.predict([[bedrooms, bathrooms]])\n",
        "    estimate = y_pred[0]\n",
        "    bed_coef = model.coef_[0]\n",
        "    bath_coef = model.coef_[1]\n",
        "    \n",
        "    # Format with $ and comma separators. No decimals.\n",
        "    result = f'Rent for a {bedrooms}-bed, {bathrooms}-bath apartment in NYC is estimated at ${estimate:,.0f}.'\n",
        "    explanation = f' In this model, each bedroom adds ${bed_coef:,.0f} & each bathroom adds ${bath_coef:,.0f}.'\n",
        "    return result + explanation\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soleTV07z65L"
      },
      "source": [
        "Let’s do something similar, but with a tuned Random Forest and Shapley Values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "1ROt_lx9z65L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read New York City apartment rental listing data\n",
        "df = pd.read_csv(DATA_PATH+'apartments/renthop-nyc.csv')\n",
        "assert df.shape == (49352, 34)\n",
        "\n",
        "# Remove the most extreme 1% prices,\n",
        "# the most extreme .1% latitudes, &\n",
        "# the most extreme .1% longitudes\n",
        "df = df[(df['price'] >= np.percentile(df['price'], 0.5)) &\n",
        "        (df['price'] <= np.percentile(df['price'], 99.5)) &\n",
        "        (df['latitude'] >= np.percentile(df['latitude'], 0.05)) &\n",
        "        (df['latitude'] < np.percentile(df['latitude'], 99.95)) &\n",
        "        (df['longitude'] >= np.percentile(df['longitude'], 0.05)) &\n",
        "        (df['longitude'] <= np.percentile(df['longitude'], 99.95))]\n",
        "\n",
        "# Do train/test split\n",
        "# Use data from April & May 2016 to train\n",
        "# Use data from June 2016 to test\n",
        "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)\n",
        "cutoff = pd.to_datetime('2016-06-01')\n",
        "train = df[df.created < cutoff]\n",
        "test  = df[df.created >= cutoff]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8A08GKz65M"
      },
      "outputs": [],
      "source": [
        "# Assign to X, y\n",
        "features = ['bedrooms', 'bathrooms', 'longitude', 'latitude']\n",
        "target = 'price'\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_test = test[features]\n",
        "y_test = test[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvTFcqimz65M"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint, uniform\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(50, 500),\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'max_features': uniform(0, 1),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=5,\n",
        "    cv=2,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    verbose=10,\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "IhhfkQUZz65N"
      },
      "outputs": [],
      "source": [
        "print('Best hyperparameters', search.best_params_)\n",
        "print('Cross-validation MAE', -search.best_score_)\n",
        "model = search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxvBHWdwz65O"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3WG35Zz65O"
      },
      "source": [
        "#### [Dan Becker explains Shapley Values:](https://www.kaggle.com/dansbecker/shap-values)\n",
        "\n",
        ">You've seen (and used) techniques to extract general insights from a machine learning model. But what if you want to break down how the model works for an individual prediction?\n",
        ">\n",
        ">SHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature.\n",
        ">\n",
        ">There is some complexity to the technique ... We won't go into that detail here, since it isn't critical for using the technique. [This blog post](https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d) has a longer theoretical explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcoj5xkdz65P"
      },
      "outputs": [],
      "source": [
        "# Get an individual observation to explain.\n",
        "# For example, the 0th row from the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJl9hoZaz65P"
      },
      "outputs": [],
      "source": [
        "# What was the actual rent for this apartment?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwBSJOXKz65Q"
      },
      "outputs": [],
      "source": [
        "# What does the model predict for this apartment?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4X_kQilz65R"
      },
      "outputs": [],
      "source": [
        "# Why did the model predict this?\n",
        "# Look at a Shapley Values Force Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCgQNCQLz65R"
      },
      "source": [
        "### Define the predict function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "jgn1gh0az65S"
      },
      "outputs": [],
      "source": [
        "def predict(bedrooms, bathrooms, longitude, latitude):\n",
        "\n",
        "    # Make dataframe from the inputs\n",
        "    df = pd.DataFrame(\n",
        "        data=[[bedrooms, bathrooms, longitude, latitude]],\n",
        "        columns=['bedrooms', 'bathrooms', 'longitude', 'latitude']\n",
        "    )\n",
        "\n",
        "    # Get the model's prediction\n",
        "    pred = model.predict(df)[0]\n",
        "\n",
        "    # Calculate shap values\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(df)\n",
        "\n",
        "    # Get series with shap values, feature names, & feature values\n",
        "    feature_names = df.columns\n",
        "    feature_values = df.values[0]\n",
        "    shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))\n",
        "\n",
        "    # Print results\n",
        "    result = f'${pred:,.0f} estimated rent for this NYC apartment. \\n\\n'\n",
        "    result += f'Starting from baseline of ${explainer.expected_value:} \\n'\n",
        "    result += shaps.to_string()\n",
        "    print(result)\n",
        "\n",
        "    # Show shapley values force plot\n",
        "    shap.initjs()\n",
        "    return shap.force_plot(\n",
        "        base_value=explainer.expected_value,\n",
        "        shap_values=shap_values,\n",
        "        features=df\n",
        "    )\n",
        "\n",
        "predict(3, 1.5, -73.9425, 40.7145)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8zjMRU1z65S"
      },
      "outputs": [],
      "source": [
        "# What if it was a 2 bedroom?\n",
        "predict(2, 1.5, -73.9425, 40.7145)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-yanGbuz65T"
      },
      "outputs": [],
      "source": [
        "# What if it was a 1 bedroom?\n",
        "predict(1, 1.5, -73.9425, 40.7145)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o0Hcyzdz65T"
      },
      "source": [
        "### BONUS: Classification example\n",
        "\n",
        "Just for your future reference, here's a bonus example for a classification problem. This uses Lending Club data, historical and current. The goal: Predict if peer-to-peer loans are charged off or fully paid. Decide which loans to invest in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCP9vcQYz65T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Stratified sample, 10% of expired Lending Club loans, grades A-D\n",
        "# Source: https://www.lendingclub.com/info/download-data.action\n",
        "history = pd.read_csv(DATA_PATH+'lending-club/lending-club-subset.csv')\n",
        "history['issue_d'] = pd.to_datetime(history['issue_d'], infer_datetime_format=True)\n",
        "\n",
        "# Current loans available for manual investing, June 17, 2019\n",
        "# Source: https://www.lendingclub.com/browse/browse.action\n",
        "current = pd.read_csv(DATA_PATH+'../data/lending-club/primaryMarketNotes_browseNotes_1-RETAIL.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awj9UkPLz65U"
      },
      "outputs": [],
      "source": [
        "# Transform earliest_cr_line to an integer:\n",
        "# How many days the earliest credit line was open, before the loan was issued.\n",
        "# For current loans available for manual investing, assume the loan will be issued today.\n",
        "history['earliest_cr_line'] = pd.to_datetime(history['earliest_cr_line'], infer_datetime_format=True)\n",
        "history['earliest_cr_line'] = history['issue_d'] - history['earliest_cr_line']\n",
        "history['earliest_cr_line'] = history['earliest_cr_line'].dt.days\n",
        "\n",
        "current['earliest_cr_line'] = pd.to_datetime(current['earliest_cr_line'], infer_datetime_format=True)\n",
        "current['earliest_cr_line'] = pd.Timestamp.today() - current['earliest_cr_line']\n",
        "current['earliest_cr_line'] = current['earliest_cr_line'].dt.days\n",
        "\n",
        "# Transform earliest_cr_line for the secondary applicant\n",
        "history['sec_app_earliest_cr_line'] = pd.to_datetime(history['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
        "history['sec_app_earliest_cr_line'] = history['issue_d'] - history['sec_app_earliest_cr_line']\n",
        "history['sec_app_earliest_cr_line'] = history['sec_app_earliest_cr_line'].dt.days\n",
        "\n",
        "current['sec_app_earliest_cr_line'] = pd.to_datetime(current['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
        "current['sec_app_earliest_cr_line'] = pd.Timestamp.today() - current['sec_app_earliest_cr_line']\n",
        "current['sec_app_earliest_cr_line'] = current['sec_app_earliest_cr_line'].dt.days\n",
        "\n",
        "# Engineer features for issue date year & month\n",
        "history['issue_d_year'] = history['issue_d'].dt.year\n",
        "history['issue_d_month'] = history['issue_d'].dt.month\n",
        "\n",
        "current['issue_d_year'] = pd.Timestamp.today().year\n",
        "current['issue_d_month'] = pd.Timestamp.today().month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1koeiqByz65U"
      },
      "outputs": [],
      "source": [
        "# Calculate percent of each loan repaid\n",
        "history['percent_paid'] = history['total_pymnt'] / history['funded_amnt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itBTAAZmz65V"
      },
      "outputs": [],
      "source": [
        "# Train on the historical data.\n",
        "# For the target, use `loan_status` ('Fully Paid' or 'Charged Off')\n",
        "target = 'loan_status'\n",
        "X = history.drop(columns=target)\n",
        "y = history[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu9DL8PMz65V"
      },
      "outputs": [],
      "source": [
        "# Do train/validate/test 3-way split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=20000, stratify=y, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=20000,\n",
        "    stratify=y_trainval, random_state=42)\n",
        "\n",
        "print('X_train shape', X_train.shape)\n",
        "print('y_train shape', y_train.shape)\n",
        "print('X_val shape', X_val.shape)\n",
        "print('y_val shape', y_val.shape)\n",
        "print('X_test shape', X_test.shape)\n",
        "print('y_test shape', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDHQfHLcz65V"
      },
      "outputs": [],
      "source": [
        "# Save the ids for later, so we can look up actual results,\n",
        "# to compare with predicted results\n",
        "train_id = X_train['id']\n",
        "val_id = X_val['id']\n",
        "test_id = X_test['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3u3C_JLz65W"
      },
      "outputs": [],
      "source": [
        "# Use Python sets to compare the historical columns & current columns\n",
        "common_columns = set(history.columns) & set(current.columns)\n",
        "just_history = set(history.columns) - set(current.columns)\n",
        "just_current = set(current.columns) - set(history.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvdR3QNoz65W"
      },
      "outputs": [],
      "source": [
        "# For features, use only the common columns shared by the historical & current data.\n",
        "features = list(common_columns)\n",
        "X_train = X_train[features]\n",
        "X_val = X_val[features]\n",
        "X_test = X_test[features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "297TPVxFz65W"
      },
      "outputs": [],
      "source": [
        "def wrangle(X):\n",
        "    X = X.copy()\n",
        "\n",
        "    # Engineer new feature for every feature: is the feature null?\n",
        "    for col in X:\n",
        "        X[col+'_NULL'] = X[col].isnull()\n",
        "\n",
        "    # Convert percentages from strings to floats\n",
        "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
        "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
        "\n",
        "    # Convert employment length from string to float\n",
        "    X['emp_length'] = X['emp_length'].str.replace(r'\\D','').astype(float)\n",
        "\n",
        "    # Create features for three employee titles: teacher, manager, owner\n",
        "    X['emp_title'] = X['emp_title'].str.lower()\n",
        "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
        "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
        "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
        "\n",
        "    # Get length of free text fields\n",
        "    X['title'] = X['title'].str.len()\n",
        "    X['desc'] = X['desc'].str.len()\n",
        "    X['emp_title'] = X['emp_title'].str.len()\n",
        "\n",
        "    # Convert sub_grade from string \"A1\"-\"D5\" to numbers\n",
        "    sub_grade_ranks = {'A1': 1.1, 'A2': 1.2, 'A3': 1.3, 'A4': 1.4, 'A5': 1.5,\n",
        "                       'B1': 2.1, 'B2': 2.2, 'B3': 2.3, 'B4': 2.4, 'B5': 2.5,\n",
        "                       'C1': 3.1, 'C2': 3.2, 'C3': 3.3, 'C4': 3.4, 'C5': 3.5,\n",
        "                       'D1': 4.1, 'D2': 4.2, 'D3': 4.3, 'D4': 4.4, 'D5': 4.5}\n",
        "    X['sub_grade'] = X['sub_grade'].map(sub_grade_ranks)\n",
        "\n",
        "    # Drop some columns\n",
        "    X = X.drop(columns='id')        # Always unique\n",
        "    X = X.drop(columns='url')       # Always unique\n",
        "    X = X.drop(columns='member_id') # Always null\n",
        "    X = X.drop(columns='grade')     # Duplicative of sub_grade\n",
        "    X = X.drop(columns='zip_code')  # High cardinality\n",
        "\n",
        "    # Only use these features which had nonzero permutation importances in earlier models\n",
        "    features = ['acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc',\n",
        "                'annual_inc_joint', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
        "                'collections_12_mths_ex_med', 'delinq_amnt', 'desc_NULL', 'dti',\n",
        "                'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_length_NULL',\n",
        "                'emp_title', 'emp_title_NULL', 'emp_title_owner', 'fico_range_high',\n",
        "                'funded_amnt', 'home_ownership', 'inq_last_12m', 'inq_last_6mths',\n",
        "                'installment', 'int_rate', 'issue_d_month', 'issue_d_year', 'loan_amnt',\n",
        "                'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op',\n",
        "                'mo_sin_rcnt_rev_tl_op', 'mort_acc', 'mths_since_last_major_derog_NULL',\n",
        "                'mths_since_last_record', 'mths_since_recent_bc', 'mths_since_recent_inq',\n",
        "                'num_actv_bc_tl', 'num_actv_rev_tl', 'num_op_rev_tl', 'num_rev_tl_bal_gt_0',\n",
        "                'num_tl_120dpd_2m_NULL', 'open_rv_12m_NULL', 'open_rv_24m',\n",
        "                'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'purpose',\n",
        "                'revol_bal', 'revol_bal_joint', 'sec_app_earliest_cr_line',\n",
        "                'sec_app_fico_range_high', 'sec_app_open_acc', 'sec_app_open_act_il',\n",
        "                'sub_grade', 'term', 'title', 'title_NULL', 'tot_coll_amt',\n",
        "                'tot_hi_cred_lim', 'total_acc', 'total_bal_il', 'total_bc_limit',\n",
        "                'total_cu_tl', 'total_rev_hi_lim']\n",
        "    X = X[features]\n",
        "\n",
        "    # Reset index\n",
        "    X = X.reset_index(drop=True)\n",
        "\n",
        "    # Return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "X_train = wrangle(X_train)\n",
        "X_val   = wrangle(X_val)\n",
        "X_test  = wrangle(X_test)\n",
        "\n",
        "print('X_train shape', X_train.shape)\n",
        "print('X_val shape', X_val.shape)\n",
        "print('X_test shape', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw9ioHyZz65X"
      },
      "outputs": [],
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "processor = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy='median')\n",
        ")\n",
        "\n",
        "X_train_processed = processor.fit_transform(X_train)\n",
        "X_val_processed = processor.transform(X_val)\n",
        "\n",
        "eval_set = [(X_train_processed, y_train),\n",
        "            (X_val_processed, y_val)]\n",
        "\n",
        "model = XGBClassifier(n_estimators=1000, n_jobs=-1)\n",
        "model.fit(X_train_processed, y_train, eval_set=eval_set, eval_metric='auc',\n",
        "          early_stopping_rounds=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "MUAllFhdz65X"
      },
      "outputs": [],
      "source": [
        "# THIS CELL ISN'T ABOUT THE NEW OBJECTIVES FOR TODAY\n",
        "# BUT IT IS IMPORTANT FOR YOUR SPRINT CHALLENGE\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "X_test_processed = processor.transform(X_test)\n",
        "class_index = 1\n",
        "y_pred_proba = model.predict_proba(X_test_processed)[:, class_index]\n",
        "print(f'Test ROC AUC for class {class_index}:')\n",
        "print(roc_auc_score(y_test, y_pred_proba)) # Ranges from 0-1, higher is better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn5IVnuuz65Y"
      },
      "source": [
        "#### Look at predictions vs actuals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHpFnU61z65Y"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'pred_proba': y_pred_proba,\n",
        "    'status_group': y_test\n",
        "})\n",
        "\n",
        "df = df.merge(\n",
        "     history[['id', 'issue_d', 'sub_grade', 'percent_paid', 'term', 'int_rate']],\n",
        "     how='left'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_jOBYNyz65Y"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "xXTarsoIz65Z"
      },
      "outputs": [],
      "source": [
        "fully_paid = df['status_group'] == 'Fully Paid'\n",
        "charged_off = ~fully_paid\n",
        "right = (fully_paid) == (df['pred_proba'] > 0.50)\n",
        "wrong = ~right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEoqAludz65Z"
      },
      "source": [
        "#### Loan was fully paid, model's prediction was right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "0UuBZdDIz65Z"
      },
      "outputs": [],
      "source": [
        "df[fully_paid & right].sample(n=10, random_state=1).sort_values(by='pred_proba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRQNLH-7z65Z"
      },
      "outputs": [],
      "source": [
        "# To explain the prediction for test observation with index #3094,\n",
        "# first, get all of the features for that observation\n",
        "row = X_test.iloc[[3094]]\n",
        "row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BorONmvuz65a"
      },
      "source": [
        "#### Explain individual predictions with shapley value plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vMJkeq0z65a"
      },
      "outputs": [],
      "source": [
        "# STUDY/PRACTICE THIS CELL FOR THE SPRINT CHALLENGE\n",
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "row_processed = processor.transform(row)\n",
        "shap_values = explainer.shap_values(row_processed)\n",
        "\n",
        "shap.initjs()\n",
        "shap.force_plot(\n",
        "    base_value=explainer.expected_value,\n",
        "    shap_values=shap_values,\n",
        "    features=row,\n",
        "    link='logit' # For classification, this shows predicted probabilities\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWXe2jMqz65a"
      },
      "source": [
        "#### Make a function to explain predictions\n",
        "\n",
        "Goal Output:\n",
        "\n",
        "```\n",
        "The model predicts this loan is Fully Paid, with 74% probability.\n",
        "\n",
        "\n",
        "Top 3 reasons for prediction:\n",
        "1. dti is 10.97.\n",
        "2. term is  36 months.\n",
        "3. total_acc is 45.0.\n",
        "\n",
        "\n",
        "Top counter-argument against prediction:\n",
        "- sub_grade is 4.2.\n",
        "\n",
        "<INSERT SHAPLEY VALUE FORCE PLOT HERE>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzSKqMrEz65b"
      },
      "outputs": [],
      "source": [
        "feature_names = row.columns\n",
        "feature_values = row.values[0]\n",
        "shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unEHBjBuz65b"
      },
      "outputs": [],
      "source": [
        "pros = shaps.sort_values(ascending=False)[:3].index\n",
        "cons = shaps.sort_values(ascending=True)[:3].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lzHIU5Zz65c"
      },
      "outputs": [],
      "source": [
        "print('Top 3 reasons for fully paid:')\n",
        "for i, pro in enumerate(pros, start=1):\n",
        "    feature_name, feature_value = pro\n",
        "    print(f'{i}. {feature_name} is {feature_value}.')\n",
        "\n",
        "print('\\n')\n",
        "print('Cons:')\n",
        "for i, con in enumerate(cons, start=1):\n",
        "    feature_name, feature_value = con\n",
        "    print(f'{i}. {feature_name} is {feature_value}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwQ0kWoNz65c"
      },
      "outputs": [],
      "source": [
        "def explain(row_number):\n",
        "    positive_class = 'Fully Paid'\n",
        "    positive_class_index = 1\n",
        "\n",
        "    # Get & process the data for the row\n",
        "    row = X_test.iloc[[row_number]]\n",
        "    row_processed = processor.transform(row)\n",
        "\n",
        "    # Make predictions (includes predicted probability)\n",
        "    pred = model.predict(row_processed)[0]\n",
        "    pred_proba = model.predict_proba(row_processed)[0, positive_class_index]\n",
        "    pred_proba *= 100\n",
        "    if pred != positive_class:\n",
        "        pred_proba = 100 - pred_proba\n",
        "\n",
        "    # Show prediction & probability\n",
        "    print(f'The model predicts this loan is {pred}, with {pred_proba:.0f}% probability.')\n",
        "\n",
        "    # Get shapley additive explanations\n",
        "    shap_values = explainer.shap_values(row_processed)\n",
        "\n",
        "    # Get top 3 \"pros & cons\" for fully paid\n",
        "    feature_names = row.columns\n",
        "    feature_values = row.values[0]\n",
        "    shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))\n",
        "    pros = shaps.sort_values(ascending=False)[:3].index\n",
        "    cons = shaps.sort_values(ascending=True)[:3].index\n",
        "\n",
        "    # Show top 3 reason for prediction\n",
        "    print('\\n')\n",
        "    print('Top 3 reasons for prediction:')\n",
        "    evidence = pros if pred == positive_class else cons\n",
        "    for i, info in enumerate(evidence, start=1):\n",
        "        feature_name, feature_value = info\n",
        "        print(f'{i}. {feature_name} is {feature_value}.')\n",
        "\n",
        "    # Show top 1 counter-argument against prediction\n",
        "    print('\\n')\n",
        "    print('Top counter-argument against prediction:')\n",
        "    evidence = cons if pred == positive_class else pros\n",
        "    feature_name, feature_value = evidence[0]\n",
        "    print(f'- {feature_name} is {feature_value}.')\n",
        "\n",
        "    # Show Shapley Values Force Plot\n",
        "    shap.initjs()\n",
        "    return shap.force_plot(\n",
        "        base_value=explainer.expected_value,\n",
        "        shap_values=shap_values,\n",
        "        features=row,\n",
        "        link='logit' # For classification, this shows predicted probabilities\n",
        "    )\n",
        "\n",
        "explain(3094)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvvYTPDNz65d"
      },
      "source": [
        "#### Look at more examples\n",
        "\n",
        "You can choose an example from each quadrant of the confusion matrix, and get an explanation for the model's prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZbYy7gWz65d"
      },
      "source": [
        "#### Loan was charged off, model's prediction was right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "GPL1A61vz65d"
      },
      "outputs": [],
      "source": [
        "df[charged_off & right].sample(n=10, random_state=1).sort_values(by='pred_proba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovOHBBKfz65e"
      },
      "outputs": [],
      "source": [
        "explain(8383)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5e4s-Brz65e"
      },
      "source": [
        "#### Loan was fully paid, model's prediction was wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "YalkCTDfz65f"
      },
      "outputs": [],
      "source": [
        "df[fully_paid & wrong].sample(n=10, random_state=1).sort_values(by='pred_proba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVmZ_ySaz65f"
      },
      "outputs": [],
      "source": [
        "explain(18061)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHcszADmz65f"
      },
      "outputs": [],
      "source": [
        "explain(6763)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJRX85rtz65g"
      },
      "source": [
        "#### Loan was charged off, model's prediction was wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlBUJQDPz65g"
      },
      "outputs": [],
      "source": [
        "df[charged_off & wrong].sample(n=10, random_state=1).sort_values(by='pred_proba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjcSMG_Kz65h"
      },
      "outputs": [],
      "source": [
        "explain(19883)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JVqsw9Nz65h"
      },
      "source": [
        "## References\n",
        "\n",
        "#### Partial Dependence Plots\n",
        "- [Kaggle / Dan Becker: Machine Learning Explainability — Partial Dependence Plots](https://www.kaggle.com/dansbecker/partial-plots)\n",
        "- [Christoph Molnar: Interpretable Machine Learning — Partial Dependence Plots](https://christophm.github.io/interpretable-ml-book/pdp.html) + [animated explanation](https://twitter.com/ChristophMolnar/status/1066398522608635904)\n",
        "- [pdpbox repo](https://github.com/SauceCat/PDPbox) & [docs](https://pdpbox.readthedocs.io/en/latest/)\n",
        "\n",
        "#### Shapley Values\n",
        "- [Kaggle / Dan Becker: Machine Learning Explainability — SHAP Values](https://www.kaggle.com/learn/machine-learning-explainability)\n",
        "- [Christoph Molnar: Interpretable Machine Learning — Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)\n",
        "- [SHAP repo](https://github.com/slundberg/shap) & [docs](https://shap.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbB8PRMFz65h"
      },
      "source": [
        "## Recap\n",
        "\n",
        "You learned about three types of model explanations during the past 1.5 lessons:\n",
        "\n",
        "#### 1. Global model explanation: all features in relation to each other\n",
        "- Feature Importances: _Default, fastest, good for first estimates_\n",
        "- Drop-Column Importances: _The best in theory, but much too slow in practice_\n",
        "- Permutaton Importances: _A good compromise!_\n",
        "\n",
        "#### 2. Global model explanation: individual feature(s) in relation to target\n",
        "- Partial Dependence plots\n",
        "\n",
        "#### 3. Individual prediction explanation\n",
        "- Shapley Values\n",
        "\n",
        "_Note that the coefficients from a linear model give you all three types of explanations!_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2aQBADcz65i"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Complete these tasks for your project, and document your work.\n",
        "\n",
        "- Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.\n",
        "- Make at least 1 partial dependence plot to explain your model.\n",
        "- Make at least 1 Shapley force plot to explain an individual prediction.\n",
        "- **Share at least 1 visualization (of any type) on Slack!**\n",
        "\n",
        "If you aren't ready to make these plots with your own dataset, you can practice these objectives with any dataset you've worked with previously. Example solutions are available for Partial Dependence Plots with the Tanzania Waterpumps dataset, and Shapley force plots with the Titanic dataset. (These datasets are available in the data directory of this repository.)\n",
        "\n",
        "Please be aware that **multi-class classification** will result in multiple Partial Dependence Plots (one for each class), and multiple sets of Shapley Values (one for each class).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}