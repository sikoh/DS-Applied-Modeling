{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hTictxWYih7"
      },
      "source": [
        "# Permutation & Boosting\n",
        "\n",
        "- Get **permutation importances** for model interpretation and feature selection\n",
        "- Use xgboost for **gradient boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmaaCmCprYw"
      },
      "source": [
        "# Downloading the Tanzania Waterpump Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkdEatMkUKa1"
      },
      "source": [
        "Make sure  you only use the dataset that is available through the **DS** **Kaggle Competition**. DO NOT USE any other Tanzania waterpump datasets that you might find online.\n",
        "\n",
        "There are two ways you can get the dataset. Make sure you have joined the competition first!:\n",
        "\n",
        "1. You can download the dataset directly by accessing the challenge and the files through the Kaggle Competition URL on Canvas (make sure you have joined the competition!)\n",
        "\n",
        "2. Use the Kaggle API using the code in the following cells. This article provides helpful information on how to fetch your Kaggle Dataset into Google Colab using the Kaggle API.\n",
        "\n",
        "> https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6TZ5nDFYkCa"
      },
      "source": [
        "# Using Kaggle API to download datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2e6fPUATxLZ",
        "outputId": "8099bdf3-dfcb-4976-f065-581bf483fd26"
      },
      "source": [
        "# mounting your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYSpUv9uYBAo",
        "outputId": "3d6b9a89-96ef-4c3e-b4b2-1adc1e7a3e35"
      },
      "source": [
        "#change your working directory, if you want to or have already saved your kaggle dataset on google drive.\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "# update it to your folder location on drive that contains the dataset and/or kaggle API token json file."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXChvgdZYb_t"
      },
      "source": [
        "# Download your Kaggle Dataset, if you haven't already done so.\n",
        "# import os\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "# !kaggle competitions download -c bloomtech-water-pump-challenge # double check if this command corresponds with the one given in Canvas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB84qgRRYdDF"
      },
      "source": [
        "# Unzip your Kaggle dataset, if you haven't already done so.\n",
        "# !unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eag2zYiQYf6q",
        "outputId": "7245f6d5-1841-4d8a-d7fb-b84710a7eae9"
      },
      "source": [
        "# List all files in your Kaggle folder on your google drive.\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_submission.csv  test_features.csv  train_features.csv  train_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKGp79CErOo-"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNKAp0MFnXW"
      },
      "source": [
        "%%capture\n",
        "!pip install category_encoders==2.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLiyUNS0Sy-i",
        "ExecuteTime": {
          "end_time": "2023-07-07T14:15:45.558935Z",
          "start_time": "2023-07-07T14:15:43.661363Z"
        }
      },
      "source": [
        "# data analysis and wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# encoders\n",
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Bagged Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Boosted Models\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Permutation Importance\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# for displaying images and html\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfhziD2Wn_iO"
      },
      "source": [
        "# Wrangle Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AomWBQncSy-j"
      },
      "source": [
        "We'll go back to Tanzania Waterpumps for this lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-TExplb_Slf"
      },
      "source": [
        "def wrangle(fm_path, tv_path=None):\n",
        "    if tv_path:\n",
        "        df = pd.merge(pd.read_csv(fm_path, parse_dates=['date_recorded'], na_values=[-2e-08, 0]),\n",
        "                         pd.read_csv(tv_path)).set_index('date_recorded')\n",
        "\n",
        "    else:\n",
        "        df = pd.read_csv(fm_path, parse_dates=['date_recorded']).set_index('date_recorded')\n",
        "\n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    # Also create a \"missing indicator\" column, because of the fact that\n",
        "    # values are missing may be a predictive signal.\n",
        "    cols_with_zeros = ['longitude', 'latitude', 'construction_year',\n",
        "                       'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        df[col] = df[col].replace(0, np.nan)\n",
        "        df[col+'_MISSING'] = df[col].isnull()\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    duplicates = ['quantity_group', 'payment_type']\n",
        "    df = df.drop(columns=duplicates)\n",
        "\n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    df = df.drop(columns=unusable_variance)\n",
        "\n",
        "    # Extract components from date_recorded\n",
        "    df['year_recorded'] = df.index.year\n",
        "    df['month_recorded'] = df.index.month\n",
        "    df['day_recorded'] = df.index.day\n",
        "\n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    df['pump_age'] = df['year_recorded'] - df['construction_year']\n",
        "    df['years_MISSING'] = df['pump_age'].isnull()\n",
        "\n",
        "    # return the wrangled dataframe\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ik6YLCOSy-j"
      },
      "source": [
        "fm_path = 'train_features.csv'\n",
        "tv_path = 'train_labels.csv'\n",
        "\n",
        "df = wrangle(fm_path, tv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSCa9nPnSy-k"
      },
      "source": [
        "# Split data into feature and target\n",
        "target = 'status_group'\n",
        "X, y = df.drop(columns = target), df[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYLn6NLESy-k"
      },
      "source": [
        "# Split data into train, validation and test sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxOEAYnUSy-k"
      },
      "source": [
        "# Baseline Metric\n",
        "print('The baseline accuracy is ', y_train.value_counts(normalize=True).max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3jiNoOEgIbi"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "\n",
        "model_rf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(strategy='median'),\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1,n_estimators=75)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "model_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "9ttetGUElX8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosted Model\n",
        "\n"
      ],
      "metadata": {
        "id": "5nGicyaLlX3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGB Model\n",
        "\n"
      ],
      "metadata": {
        "id": "G9jhk-ylPJ8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfmiBiimgIbj"
      },
      "source": [
        "# Check Metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training Accuracy', model_rf.score(X_train, y_train))\n",
        "print('Validation Accuracy', model_rf.score(X_val, y_val))"
      ],
      "metadata": {
        "id": "9r2ePpByPSZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8lB4z5l_eml"
      },
      "source": [
        "print('Training Accuracy', model_gb.score(X_train, y_train))\n",
        "print('Validation Accuracy', model_gb.score(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmLrHQlMSy-m"
      },
      "source": [
        "print('Training Accuracy', model_xgb.score(X_train, y_train))\n",
        "print('Validation Accuracy', model_xgb.score(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7-ml6BhRRf"
      },
      "source": [
        "### Try adjusting these hyperparameters\n",
        "\n",
        "#### Random Forest\n",
        "- class_weight (for imbalanced classes)\n",
        "- max_depth (usually high, can try decreasing)\n",
        "- n_estimators (too low underfits, too high wastes time)\n",
        "- min_samples_leaf (increase if overfitting)\n",
        "- max_features (decrease for more diverse trees)\n",
        "\n",
        "#### Xgboost\n",
        "- scale_pos_weight (for imbalanced classes)\n",
        "- max_depth (usually low, can try increasing)\n",
        "- n_estimators (too low underfits, too high wastes time/overfits) — Use Early Stopping!\n",
        "- learning_rate (too low underfits, too high overfits)\n",
        "\n",
        "For more ideas, see [Notes on Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html) and [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djCjZ5WBgIbk"
      },
      "source": [
        "# Tuning / Communication\n",
        "\n",
        "- How can we determine or communicate which features are most important to our model when making predictions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvFQRNbSTwle"
      },
      "source": [
        "**Option 1:** Grab feature importances from our pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgTWgFV6MaTW"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "feat_imp.tail(10).plot(kind='barh')\n",
        "plt.xlabel('Gini Importance')\n",
        "plt.ylabel('Feature')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN3aL3FeT-fC"
      },
      "source": [
        "**Option 2:** Drop-column Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_column ="
      ],
      "metadata": {
        "id": "bo-aZhrYmdL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_col = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    XGBClassifier(n_estimators=75,\n",
        "                  random_state=42,\n",
        "                  n_jobs=-1,)\n",
        ")\n",
        "\n",
        "model_with_col.fit(X_train, y_train)\n",
        "\n",
        "print(f'Validation Accuracy w/ \"{selected_column}\" included:', model_with_col.score(X_val, y_val))"
      ],
      "metadata": {
        "id": "eohb8YLqmdDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Validation Accuracy w/ \"{selected_column}\" excluded:', )"
      ],
      "metadata": {
        "id": "bda1KdZZmcqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ya23IUpmcjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLhviblHgIbn"
      },
      "source": [
        "**Option 3:** Permutation Importance\n",
        "\n",
        "![](https://i.imgur.com/h17tMUU.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# By hand\n",
        "\n",
        "# Step 1: Choose my feature\n",
        "column_to_permute =\n",
        "\n",
        "# Step 2: Train model w/ ALL features\n",
        "model_to_permute = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    XGBClassifier(n_estimators=75,\n",
        "                  random_state=42,\n",
        "                  n_jobs=-1,)\n",
        ")\n",
        "\n",
        "model_to_permute.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "NESnUf2WQQTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Evaluate model using VALIDATION DATA.\n",
        "print('Validation Accuracy', model_to_permute.score(X_val, y_val))"
      ],
      "metadata": {
        "id": "4ux4hkWWQQJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: In VALIDATION DATA, permute the feature we're evaluating\n"
      ],
      "metadata": {
        "id": "h1hPgbaHQaF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Calculate our error metric with the permuted data\n",
        "print('Validation Accuracy', model_to_permute.score(, y_val))"
      ],
      "metadata": {
        "id": "I2Q-EQZMQeFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automated using sklearn\n"
      ],
      "metadata": {
        "id": "w2qYc0aAP9pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWrWCtO4P9mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_perm = {'imp_mean':perm_imp['importances_mean'],\n",
        "             'imp_std':perm_imp['importances_std']}\n",
        "df_perm = pd.DataFrame(data_perm, index=X_val.columns).sort_values('imp_mean')"
      ],
      "metadata": {
        "id": "ocMn9FFDP-AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_perm"
      ],
      "metadata": {
        "id": "MfRMVXpFP_uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_perm['imp_mean'].tail(10).plot(kind='barh')\n"
      ],
      "metadata": {
        "id": "pOt63i7vQEoC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}